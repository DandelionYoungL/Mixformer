task: language_modeling
rank-list-size: 150
ranking-patience: 5
corr-threshold: 0.85

# model
arch: transformer_lm_wiki103_super
data: ../data-bin/wikitext-103
max-relative-length: -1

# training settings
max-batch: 1500
max-update: 286000
max-lr: 0.1
t-mult: 2
lr-period-updates: 270000
lr-scheduler: cosine
lr-shrink: 0.75
warmup-updates: 16000
warmup-init-lr: 1e-07
min-lr: 1e-09
optimizer: nag
lr: 0.0001
clip-norm: 0.1
criterion: adaptive_loss
max-tokens: 3072
update-freq: 3
tokens-per-sample: 2560
seed: 1
sample-break-mode: none
skip-invalid-size-inputs-valid-test: True
ddp-backend: no_c10d
log-interval: 100
fp16: True

# SuperTransformer configs
decoder-embed-dim: 1152
decoder-layers: 14
decoder-attention-heads: 16
decoder-ffn-embed-dim: 5120
qkv-dim: 1152

# SubTransformers search space
decoder-embed-choice: [ 1152 ]

decoder-ffn-embed-dim-choice: [ 5120, 4096, 3072 ]

decoder-layer-num-choice: [ 14, 12, 10]

decoder-self-attention-heads-choice: [ 16, 12, 8 ]
data-path: loss_dataset/wiki103_loss.data
candidate-size: 2000

task: language_modeling

# model
restore-file: checkpoints/wikitext-103/supertransformer/config/checkpoint_best.pt

# model
arch: transformer_lm_wiki103_super
data: ./data-bin/data_bin
max-relative-length: -1

# training settings
max-update: 286000
max-lr: 0.1
t-mult: 2
lr-period-updates: 270000
lr-scheduler: cosine
lr-shrink: 0.75
warmup-updates: 16000
warmup-init-lr: 1e-07
min-lr: 1e-09
optimizer: nag
lr: 0.0001
clip-norm: 0.1
criterion: adaptive_loss
max-tokens: 3072
update-freq: 3
tokens-per-sample: 2560
seed: 1
sample-break-mode: none
skip-invalid-size-inputs-valid-test: True
ddp-backend: no_c10d
log-interval: 100
fp16: True

# SuperTransformer configs
decoder-embed-dim: 1152
decoder-layers: 14
decoder-attention-heads: 16
decoder-ffn-embed-dim: 5120
qkv-dim: 1152

# SubTransformers search space
decoder-embed-choice: [ 1152 ]

decoder-ffn-embed-dim-choice: [ 5120, 4096, 3072 ]

decoder-layer-num-choice: [ 14, 12, 10]

decoder-self-attention-heads-choice: [ 16, 12, 8 ]
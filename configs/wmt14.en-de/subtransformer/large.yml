train-subtransformer: True

# model
arch: transformersuper_wmt_en_de
data: ../data/binary/wmt14_en_de_joined_dict
share-all-embeddings: True
share-decoder-input-output-embed: False
encoder-normalize-before: True
decoder-normalize-before: True
max-relative-length: 8

# training settings
max-update: 50000
lr: 0.001
min-lr: 1e-09
warmup-updates: 8000
warmup-init-lr: 1e-07
lr-scheduler: inverse_sqrt
clip-norm: 0.0
weight-decay: 0.0
#no-progress-bar: True
reset-optimizer: False
optimizer: radam
adam-betas: (0.9, 0.98)
dropout: 0.3
relu-dropout: 0.1
attention-dropout: 0.1
label-smoothing: 0.1
criterion: label_smoothed_cross_entropy

fp16: True
fp16-scale-window: 256
threshold-loss-scale: 0.03125
ddp-backend: no_c10d
update-freq: 16
max-tokens: 4096
distributed-world-size: 8
num-workers: 2
keep-last-epochs: 10
save-interval: 1
validate-interval: 1
log-interval: 100
max-tokens-valid: 4096

# SuperTransformer configs

# We train the SubTransformer inside the SuperTransformer, so need to specify a SuperTransformer
# From algorithm side, we can train a totally standalone SubTransformer and it is unnecessary to specify a SuperTransformer
# However, from implementation side, it is easier to do a Subtransformer training by always sampling the same desired SubTransformer from a specified SuperTransformer

encoder-embed-dim: 768
decoder-embed-dim: 768

encoder-ffn-embed-dim: 4096
decoder-ffn-embed-dim: 4096

encoder-layers: 6
decoder-layers: 6

encoder-attention-heads: 12
decoder-attention-heads: 12

qkv-dim: 768

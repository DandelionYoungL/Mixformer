candidate-size: 2000

# path
restore-file: checkpoints/wmt14.en-de/supertransformer/large/checkpoint_best.pt
data: ../data/binary/wmt16_en_de
data-path: ./loss_dataset/wmt14_large_loss.data

# model
arch: transformersuper_wmt_en_de
share-all-embeddings: True
share-decoder-input-output-embed: False
encoder-normalize-before: True
decoder-normalize-before: True
max-relative-length: 8

# training settings
max-update: 50000
lr: 0.001
min-lr: 1e-09
warmup-updates: 8000
warmup-init-lr: 1e-07
lr-scheduler: inverse_sqrt
clip-norm: 0.0
weight-decay: 0.0
#no-progress-bar: True
reset-optimizer: True
optimizer: radam
adam-betas: (0.9, 0.98)
dropout: 0.3
relu-dropout: 0.1
attention-dropout: 0.1
# label-smoothing: 0.1

fp16: True
fp16-scale-window: 256
threshold-loss-scale: 0.03125
ddp-backend: no_c10d
update-freq: 16
max-tokens: 4096
distributed-world-size: 8
num-workers: 2
keep-last-epochs: 10
save-interval: 1
validate-interval: 1
log-interval: 100
max-tokens-valid: 4096

# SuperTransformer configs
encoder-embed-dim: 1024
decoder-embed-dim: 1024

encoder-ffn-embed-dim: 5120
decoder-ffn-embed-dim: 5120

encoder-layers: 6
decoder-layers: 6

encoder-attention-heads: 8
decoder-attention-heads: 8

qkv-dim: 1024

# SubTransformers search space
encoder-embed-choice: [ 1024, 768, 640 ]
decoder-embed-choice: [ 1024, 768, 640 ]

encoder-ffn-embed-dim-choice: [ 5120, 4096, 3072, 2048 ]
decoder-ffn-embed-dim-choice: [ 5120, 4096, 3072, 2048 ]

encoder-layer-num-choice: [ 6 ]
decoder-layer-num-choice: [ 6, 5, 4]

encoder-self-attention-heads-choice: [ 16, 8, 4 ]
decoder-self-attention-heads-choice: [ 16, 8, 4 ]
decoder-ende-attention-heads-choice: [ 16, 8, 4 ]

# for arbitrary encoder decoder attention. -1 means attending to last one encoder layer
# 1 means last two encoder layers, 2 means last three encoder layers
decoder-arbitrary-ende-attn-choice: [ -1, 1, 2]
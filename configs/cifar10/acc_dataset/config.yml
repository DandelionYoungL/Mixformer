candidate-size: 1000
task: classification
# path
restore-file: checkpoints/cifar10/supertransformer/large/checkpoint_best.pt
data-path: ./acc_dataset/cifar10_loss.data
data: data


## data augmentation
#augmentation: True
#augmentation-schema: cut_off
#augmentation-masking-schema: word
#augmentation-masking-probability: 0.05
#augmentation-replacing-schema: mask
criterion: label_smoothed_cross_entropy

# model
arch: transformersuper_cifar10_small
#share-all-embeddings: True
#share-decoder-input-output-embed: False
encoder-normalize-before: True
#decoder-normalize-before: True
max-relative-length: 8
num-workers: 2
valid-cnt-max: 100000
# evaluation settings
max-tokens: 4096
fp16: True
#seed: 1
reset-optimizer: True

# SuperTransformer configs
encoder-embed-dim: 640
#decoder-embed-dim: 640

encoder-ffn-embed-dim: 2048
#decoder-ffn-embed-dim: 2048

encoder-layers: 6
decoder-layers: 0

encoder-attention-heads: 8
#decoder-attention-heads: 8

qkv-dim: 512

# SubTransformers search space
encoder-rpr-choice: [16, 12, 8]
#decoder-rpr-choice: [16, 12, 8]
encoder-embed-choice: [640, 512]
#decoder-embed-choice: [640, 512]

encoder-ffn-embed-dim-choice: [2048, 1536, 1024, 768]
#decoder-ffn-embed-dim-choice: [2048, 1536, 1024, 768]

encoder-layer-num-choice: [6]
#decoder-layer-num-choice: [6, 5, 4, 3, 2, 1]

encoder-self-attention-heads-choice: [8, 4, 2]
#decoder-self-attention-heads-choice: [8, 4, 2]
#decoder-ende-attention-heads-choice: [8, 4, 2]

# for arbitrary encoder decoder attention. -1 means attending to last one encoder layer
# 1 means last two encoder layers, 2 means last three encoder layers
#decoder-arbitrary-ende-attn-choice: [-1, 1, 2]
